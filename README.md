# Winning Technical Challenge ‚öΩü§ñ

Este proyecto es una soluci√≥n integral para el **Winning Technical Challenge**. Combina un motor de extracci√≥n de datos de f√∫tbol (Web Scraping) con una plataforma interactiva de simulaci√≥n de transferencias potenciada por Inteligencia Artificial y arquitecturas RAG.

## üöÄ Live Demo
Podes probar la aplicaci√≥n con este enlace: **[winning-black.vercel.app](https://winning-black.vercel.app/)** 

---

## üõ†Ô∏è Stack Tecnol√≥gico

### Frontend & UI
* **React + Vite:** Para una interfaz reactiva, r√°pida y moderna.
* **TailwindCSS:** Estilizado eficiente y dise√±o de interfaz profesional.

### Backend & Data Engine
* **Python + BeautifulSoup:** Scraper robusto encargado de extraer datos estructurados de dominios deportivos.
* **AWS S3:** Almacenamiento de los datasets extra√≠dos en formato CSV.

### AI & Vector Intelligence
* **Pinecone:** Base de datos vectorial utilizada para indexar los datos de los jugadores, permitiendo b√∫squedas sem√°nticas y recuperaci√≥n de contexto para la IA.
* **LangChain:** Orquestador utilizado para implementar el `CSVLoader` y gestionar el flujo de datos.
* **OpenAI SDK:** Motor de IA encargado de generar res√∫menes estrat√©gicos y an√°lisis de profundidad de plantilla.

---

## üèóÔ∏è Arquitectura de Datos (RAG)
El sistema sigue un flujo de **Generaci√≥n Aumentada por Recuperaci√≥n**:
1. **Extracci√≥n:** El script en Python captura informaci√≥n de jugadores (edad, pie, nacionalidad), transferencias y valuaciones.
2. **Persistencia:** Los datos se cargan en un bucket de **S3**.
3. **Indexaci√≥n:** LangChain procesa los archivos CSV y genera embeddings que se almacenan en **Pinecone**.
4. **Simulaci√≥n:** La IA consulta los vectores en Pinecone para ofrecer una respuesta precisa basada en el mercado real, el presupuesto de transferencias y el l√≠mite salarial ingresado.

---

## üìã Entregables del Desaf√≠o

### 1. Data Integration & Web Scraping 
* **Objetivo:** Extracci√≥n de datos de equipos y temporadas espec√≠ficas.
* **Entidades:** Jugadores, Clubes, Transferencias y Valuaciones.
* **Desaf√≠os:** Manejo de consistencia de datos y estructuras de dominios espec√≠ficos.

### 2. AI Transfer Simulator 
* **Visualizaci√≥n:** Simulaci√≥n de jugadores comprados/vendidos, lista de plantilla actual y cambios en la valoraci√≥n neta.
* **IA Component:** Generaci√≥n de un resumen de texto de la temporada y generaci√≥n de an√°lisis de plantel asistido por LLMs.

### 3.  üß† Decisiones T√©cnicas & Desaf√≠os
## Desaf√≠os de Scraping (Anti-Scraping & Inconsistencias)
* **Renderizado Din√°mico:** Se detect√≥ que los datos salariales se inyectan v√≠a JavaScript. Se migr√≥ de BeautifulSoup est√°tico a **Selenium** para asegurar la hidrataci√≥n completa del DOM antes de la extracci√≥n.
* **Sanitizaci√≥n de Datos:** Se implement√≥ una l√≥gica de limpieza para manejar caracteres Unicode (como el em-dash `‚Äî`) y formatos num√©ricos complejos en las funciones de JavaScript de la fuente, garantizando que el pipeline hacia S3/Pinecone reciba datos limpios.
* **Estructura Cambiante:** Se desarroll√≥ un sistema de indexaci√≥n din√°mica de columnas por nombre de encabezado para manejar variaciones en las tablas entre diferentes temporadas.

### Mejoras (Enhancements)
* **Automatizaci√≥n Multiclub:** A diferencia de un scraper simple, el sistema procesa m√∫ltiples clubes y un rango de 6 temporadas (2020-2025) de forma autom√°tica mediante la configuraci√≥n en `urls.json`.

### Limitaciones y Trade-offs
* **Dependencia de la Fuente:** El scraper es sensible a cambios estructurales mayores en el DOM de los sitios objetivo.
* **Costo de Latencia:** El uso de un navegador headless (Selenium) aumenta el tiempo de recolecci√≥n pero garantiza la fidelidad de los datos frente a m√©todos de request simples.

## üîë Variables de Entorno
Para ejecutar el backend (`api`) y la indexaci√≥n, se requiere un archivo `.env` con:
`OPENAI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_ENV`, `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`.
---

## ‚öôÔ∏è Instalaci√≥n Local

```bash
# 1. Clonar el repositorio
git clone [https://github.com/mauroradino/Winning](https://github.com/mauroradino/Winning)

# 2. Configurar el Frontend
cd ui
npm install
npm run dev

# 3. Ejecutar el Scraper (Python) [No necesario para la utilizaci√≥n del proyecto]
cd api
pip install -r requirements.txt
python main.py